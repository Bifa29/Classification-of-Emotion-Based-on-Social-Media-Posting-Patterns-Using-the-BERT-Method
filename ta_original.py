# -*- coding: utf-8 -*-
"""TA - Original

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1szXpgubnAcCQ9V56cYRWLXUrzt_wUEvV
"""

#Dataset
!gdown --id 1Z2bahf9YfvJBHOuMGsYf2Ev-JxH8fsJb

#kamus kata baku
!gdown --id 194oZ835-oFSCWpI6b5CkvGYPhLTQBOgQ

#Kamus StopWord
!gdown --id 1NL5EOgfdM76MQlmUs2XyA1i9-yKna75-

#preprocessing
!gdown --id 19anmRlaSDOlMPhIXxxNoq0hwKRXh4t9e

#kamus kata baku
!gdown --id 1I40ERrJs12gvY1r3RK2B1HshRPuaIxWu

!pip install transformers torch nltk Sastrawi
!pip install scikit-learn

import pandas as pd
import re
import nltk
from transformers import BertTokenizer, BertForSequenceClassification, AdamW, AdamWeightDecay, Adafactor
import torch
from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
import numpy as np
import matplotlib.pyplot as plt #visualiasi data
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator #visualisasi wordcloud (menampilkan kata yang paling banyak digunakan)
from sklearn.feature_extraction.text import CountVectorizer #ambil vector jarak antar kata untuk n gram
from nltk.corpus import stopwords
import string

import matplotlib.pyplot as plt

# Data
labels = ['Happy', 'Angry', 'Sad', 'Fear']
values = [2723, 2492, 2001, 1762]

# Membuat bar plot
plt.bar(labels, values, color='pink')

# Menambahkan label dan judul
plt.xlabel('Labels')
plt.ylabel('Counts')
plt.title('LABEL COUNT')

# Menyimpan plot sebagai file gambar
file_path = "Jumlah_Data_Berdasarkan_Label.png"
plt.savefig(file_path, dpi=300, bbox_inches='tight')  # Pastikan menyimpan sebelum show()

# Menampilkan plot
plt.show()

#from google.colab import drive
#drive.mount('/content/drive')

# Load Dataset
data = pd.read_excel ('UpdateDataset.xlsx')

# Menghitung jumlah masing-masing label
label_counts = data['Label'].value_counts()

# Menampilkan hasil
print(label_counts)

data.head(5) #buat ambil data label

label_counts = data['Label'].value_counts() #untuk ambil jumlah masing masing kelas

label_counts.plot(kind='bar', color='pink') #visualiasi bar plot


plt.xlabel('Labels')
plt.ylabel('Counts')
plt.title('LABEL COUNT')
plt.show()

nltk.download('stopwords')
stop_words = set(stopwords.words('indonesian'))
text = " ".join(review for review in data.full_text) #visuliasi data yang sering muncul
stopword = set(stopwords.words('indonesian'))
stopword.update(["yg"])
wordcloud = WordCloud(stopwords = stopword).generate(text)


plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""Preprocessing"""

# 1. Case Folding
#def case_folding(text):
#    return text.lower()

#data['TEKS_clean'] = data['full_text'].apply(case_folding)
#print(data['TEKS_clean'])

data['Text Case Folding'] = data['full_text'].str.lower()

print('Case Folding Result : \n')
data.head()

# 2. Text Cleansing
def remove_tweet_special(text):
    # remove tab, new line, ans back slice
    text = text.replace('\\t'," ").replace('\\n'," ").replace('\\u'," ").replace('\\',"")
    # remove non ASCII (emoticon, chinese word, .etc)
    text = text.encode('ascii', 'replace').decode('ascii')
    # remove mention, link, hashtag
    text = ' '.join(re.sub("([@#][A-Za-z0-9]+)|(\w+:\/\/\S+)"," ", text).split())
    # remove incomplete URL
    return text.replace("http://", " ").replace("https://", " ")

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_tweet_special)

print('Cleansing (Remove Tweet Special) Result : \n')
data.head()

#remove number
def remove_number(text):
    return  re.sub(r"\d+", "", text)

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_number)

print('Cleansing (Remove Number) Result : \n')
data.head()

#remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans("","",string.punctuation))

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_punctuation)

print('Cleansing (Remove Punctuation) Result : \n')
data.head()

#remove whitespace leading & trailing
def remove_whitespace_LT(text):
    return text.strip()

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_whitespace_LT)

#remove multiple whitespace into single whitespace
def remove_whitespace_multiple(text):
    return re.sub('\s+',' ',text)

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_whitespace_multiple)

print('Cleansing (Remove Whitespace) Result : \n')
data.head()

# remove single char
def remove_singl_char(text):
    return re.sub(r"\b[a-zA-Z]\b", "", text)

data['Text Case Folding'] = data['Text Case Folding'].apply(remove_singl_char)

print('Cleansing (Remove SingleChar) Result : \n')
data.head()

# 3. Tokenisasi & Normalisasi (gunakan tokenizer BERT)
from nltk.tokenize import word_tokenize
nltk.download('punkt_tab')
# NLTK word rokenize
def word_tokenize_wrapper(text):
    return word_tokenize(text)

data['Text Tokenizing'] = data['Text Case Folding'].apply(word_tokenize_wrapper)

print('Tokenizing Result : \n')
print(data['Text Tokenizing'].head())
print('\n\n\n')

# 4. Normalisasi Teks
normalizad_word = pd.read_excel("kamuskatabaku.xlsx")

normalizad_word_dict = {}

for index, row in normalizad_word.iterrows():
    if row[0] not in normalizad_word_dict:
        normalizad_word_dict[row[0]] = row[1]

def normalized_term(document):
    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]

data['Text Normalization'] = data['Text Tokenizing'].apply(normalized_term)

data['Text Normalization'].head()

# 5. Stemming (gunakan Sastrawi)
!pip install swifter

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
import swifter


factory = StemmerFactory()
stemmer = factory.create_stemmer()


def stemmed_wrapper(term):
    return stemmer.stem(term)

term_dict = {}

for document in data['Text Normalization']:
    for term in document:
        if term not in term_dict:
            term_dict[term] = ' '

print(len(term_dict))
print("------------------------")

for term in term_dict:
    term_dict[term] = stemmed_wrapper(term)
    print(term,":" ,term_dict[term])

print(term_dict)
print("------------------------")

def get_stemmed_term(document):
    return [term_dict[term] for term in document]

data['Text Stemming'] = data['Text Normalization'].swifter.apply(get_stemmed_term)
print(data['Text Stemming'])

# 6. Stopword Removal
nltk.download('stopwords')
from nltk.corpus import stopwords
list_stopwords = stopwords.words('indonesian')


list_stopwords.extend(["yg", "dg", "rt", "dgn", "ny", 'klo',
                       'kalo', 'amp', 'biar', 'bikin', 'bilang',
                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih',
                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya',
                       'jd', 'jgn', 'sdh', 'aja',
                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',
                       '&amp', 'yah', 'sdgkan', 'sdg', 'emg', 'sm', 'pls', 'mlu', 'ken',
                       'allah', 'brb', 'btw', 'b/c', 'cod', 'cmiiw', 'fyi',
                       'gg', 'ggwp', 'idk', 'ikr', 'lol', 'ootd', 'lmao', 'oot',
                       'pap', 'otw', 'tfl', 'vc', 'ygy'])


txt_stopword = pd.read_csv("stopwordbahasa.txt", names= ["stopwords"], header = None)


list_stopwords.extend(txt_stopword["stopwords"][0].split(' '))
list_stopwords = set(list_stopwords)


def stopwords_removal(words):
    return [word for word in words if word not in list_stopwords]

data['TEKS_clean'] = data['Text Stemming'].apply(stopwords_removal)


print(data['TEKS_clean'].head())

# Apply preprocessing pipeline
# data['TEKS_clean'] = data['TEKS'].apply(case_folding)
# data['TEKS_clean'] = data['TEKS_clean'].apply(text_cleaning)
# data['TEKS_clean'] = data['TEKS_clean'].apply(normalize_text)
# data['TEKS_clean'] = data['TEKS_clean'].apply(stemming)
# data['TEKS_clean'] = data['TEKS_clean'].apply(remove_stopwords)

data.to_excel('data_prepocessing.xlsx', index = False)

data.columns

# Correct column names based on the image
data_columns = ["label", "tweet", "Text Case Folding", "Text Tokenizing", "Text Normalization", "Text Stemming", "TEKS_clean"]

# Load Data with comma delimiter, skip the first row, and handle quotes properly
data = pd.read_excel("data_prepocessing.xlsx", header=0)

# Assign column names
data.columns = data_columns

# Display the first few rows in table view
data.head()

#Resampling Data
# Jumlah label sebelum resampling
print("Jumlah label sebelum resampling:")
print(data['label'].value_counts())

# Resampling seperti di atas
import pandas as pd
from sklearn.utils import resample

# Target number of rows per class
target_count = 1762

# Group data by class label
grouped = data.groupby('label')

# Equalize data using resampling
# Initialize balanced_data as an empty list to store DataFrames
balanced_data = [] # Changed to an empty list

for label, group in grouped:
    if len(group) > target_count:
        # Undersample the majority class
        balanced_group = group.sample(n=target_count, random_state=42)
    elif len(group) < target_count:
        # Oversample the minority class
        balanced_group = resample(group, replace=True, n_samples=target_count, random_state=42)
    else:
        # If already balanced, keep as is
        balanced_group = group

    balanced_data.append(balanced_group)

# Concatenate balanced groups into a single dataframe
balanced_df = pd.concat(balanced_data)

# Shuffle the balanced dataset to avoid ordered grouping
# Removed 'Label' from sample as it's not a column in balanced_df
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Jumlah label setelah resampling
print("\nJumlah label setelah resampling:")
print(balanced_df['label'].value_counts())

# Tokenisasi dengan BERT
tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')

def bert_tokenization(text):
    tokens = tokenizer.tokenize(text)
    return tokens
data['TEKS_tokens'] = data['TEKS_clean'].apply(bert_tokenization)

# 7. Mengubah label menjadi angka
label_encoder = LabelEncoder()
data['LABEL_encoded'] = label_encoder.fit_transform(data['label'])

# Dataset class
class EmosiDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# 8. Split Data (Train-Test Split)
#test size adjustable
X_train, X_test, y_train, y_test = train_test_split(data['TEKS_clean'], data['LABEL_encoded'], test_size=0.1 , random_state=42)

train_dataset = EmosiDataset(X_train.tolist(), y_train.tolist())
test_dataset = EmosiDataset(X_test.tolist(), y_test.tolist())

#batch size adjustable
train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)
test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=8)

# 9. Fine-Tune BERT Model
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('gpu') #bisa diganti make gpu kalo merasa kuat

#model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', num_labels=len(label_encoder.classes_)) #model indobert
model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1',
                                                      num_labels=len(label_encoder.classes_),
                                                      hidden_dropout_prob=0.3,  # Increase the dropout probability
                                                      attention_probs_dropout_prob=0.3)  # Increase attention dropout
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)
#optimizer = Adafactor(model.parameters(), lr=2e-6, relative_step=False)

# Training function
def train_model():
    model.train()
    total_loss = 0
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()
    return total_loss / len(train_loader)

# Evaluation function
def eval_model():
    model.eval()
    predictions, true_labels = [], []
    total_loss = 0  # To calculate validation loss
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()  # Accumulate the loss
            logits = outputs.logits
            preds = torch.argmax(logits, dim=-1)
            predictions.extend(preds.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())
    avg_val_loss = total_loss / len(test_loader)  # Calculate average validation loss
    return predictions, true_labels, avg_val_loss

# Training loop
epochs = 3  # Adjustable
for epoch in range(epochs):
    train_loss = train_model()
    predictions, true_labels, val_loss = eval_model()
    accuracy = accuracy_score(true_labels, predictions) * 100
    print(f"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%")

from torch.nn.functional import softmax

# Fungsi untuk memprediksi kalimat dan menampilkan prediksi untuk setiap kelas
def predict_sentence(sentence):
    model.eval()  # Set model ke evaluation mode
    # Preprocess input kalimat
    encoding = tokenizer(sentence, truncation=True, padding=True, return_tensors='pt', max_length=512)

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits

        # Menggunakan softmax untuk mendapatkan probabilitas kelas
        probs = softmax(logits, dim=-1)
        prediction = torch.argmax(probs, dim=-1).item()

        # Menampilkan prediksi dan probabilitas untuk masing-masing kelas
        class_probabilities = probs.squeeze().cpu().numpy()  # Ambil probabilitas untuk semua kelas
        predicted_class = label_encoder.classes_[prediction]  # Mengambil label kelas berdasarkan prediksi

        print(f"Predicted Class: {predicted_class}")
        print("Class Probabilities:")
        for i, (cls, prob) in enumerate(zip(label_encoder.classes_, class_probabilities)):
            print(f"{cls}: {prob*100:.2f}%")

# Contoh penggunaan:
sentence = "aku takut"
predict_sentence(sentence)

# Evaluasi model
predictions, true_labels, _ = eval_model()

# 10. Evaluasi
report = classification_report(true_labels, predictions, target_names=label_encoder.classes_)

print("Classification Report:")
print(report)